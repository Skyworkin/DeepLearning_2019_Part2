{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "nh, bs = 50, 512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short intro: if you can get a small batch going fairly going well, you are off to a good start for super convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_learner(model_func, loss_func, data):\n",
    "    return Learner(*model_func(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner(get_model, loss_func, data)\n",
    "run = Runner([AvgStatsCallback([accuracy])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.69311296875, tensor(0.7930)]\n",
      "valid: [0.3344400146484375, tensor(0.9039)]\n",
      "train: [0.297352734375, tensor(0.9134)]\n",
      "valid: [0.2290689697265625, tensor(0.9360)]\n",
      "train: [0.23737439453125, tensor(0.9318)]\n",
      "valid: [0.19877386474609374, tensor(0.9448)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.7548671875, tensor(0.7975)]\n",
      "valid: [0.355202783203125, tensor(0.9003)]\n",
      "train: [0.3477811328125, tensor(0.8997)]\n",
      "valid: [0.291547705078125, tensor(0.9151)]\n",
      "train: [0.294677890625, tensor(0.9154)]\n",
      "valid: [0.2694351806640625, tensor(0.9209)]\n"
     ]
    }
   ],
   "source": [
    "learn = create_learner(partial(get_model, lr=0.3), loss_func, data)\n",
    "run = Runner([AvgStatsCallback([accuracy])])\n",
    "\n",
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model_func(lr=0.5): \n",
    "    return partial(get_model, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing\n",
    "\n",
    "We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    \"\"\"\n",
    "    This is to assist in visualizing what is happening\n",
    "    There's a lot of learning rates (could be different for)\n",
    "    each groups\n",
    "    \"\"\"\n",
    "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.param_groups[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self):\n",
    "        plt.plot(self.lrs)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.losses)\n",
    "\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    \"\"\"\n",
    "    Schedules any parameter, learning rate / dropout / weight decay etc.\n",
    "    \"\"\"\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_func): \n",
    "        self.pname, self.sched_func = pname, sched_func\n",
    "\n",
    "    def set_param(self):\n",
    "        \n",
    "        # param_groups (layer groups)\n",
    "        # loop through different groups of layers\n",
    "        for pg in self.opt.param_groups:\n",
    "\n",
    "            # float of how far we are through training\n",
    "            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)\n",
    "            \n",
    "    def begin_batch(self): \n",
    "        if self.in_train: \n",
    "            self.set_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple linear schedule going from start to end. It returns a function that takes a `pos` argument (going from 0 to 1) such that this function goes from `start` (at `pos=0`) to `end` (at `pos=1`) in a linear fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sched_lin(start, end):\n",
    "    \"\"\"\n",
    "    THis is a function that will return a function\n",
    "    \"\"\"\n",
    "    def _inner(start, end, pos): \n",
    "        return start + pos*(end-start)\n",
    "    \n",
    "    # returns a function that only takes position\n",
    "    return partial(_inner, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decorators\n",
    "\n",
    "This is an example of a decorator, so it can be used for any number of times. A decorator is a function that returns a function. It will pass the following function into the `@` related function. When defining custom decorators, make sure to return another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annealer(f):\n",
    "    def _inner(start, end): \n",
    "        return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): \n",
    "    return start + pos*(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift-tab works too, in Jupyter!\n",
    "# sched_lin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = sched_lin(1,2)\n",
    "f(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): \n",
    "    \"\"\"\n",
    "    returns a cosine\n",
    "    \"\"\"\n",
    "    return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "\n",
    "@annealer\n",
    "def sched_no(start, end, pos):\n",
    "    \"\"\"\n",
    "    Always returns start\n",
    "    \"\"\"\n",
    "    return start\n",
    "\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): \n",
    "    return start * (end/start) ** pos\n",
    "\n",
    "#This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xdc1dX/wPHX4bIRkOUCEffe5MqBe2tmKpmaiqts/bKstGxZablLU3E3NHN8nYFoqZk5y40LJzhQQPbm/P64V0IEGd7BOM/H4/OQ+xn3vLmPet8P53Pe5wgpJYqiKErpYWbqABRFURTjUolfURSllFGJX1EUpZRRiV9RFKWUUYlfURSllFGJX1EUpZRRiV9RFKWUUYlfURSllFGJX1EUpZQxN3UAOXF1dZVeXl6mDkNRFKXYOH78+H0ppVt+zi2Sid/Ly4tjx46ZOgxFUZRiQwhxPb/nqq4eRVGUUkYlfkVRlFJGJX5FUZRSRiV+RVGUUkYlfkVRlFImz8QvhKgshPhDCHFOCHFWCPFmDucIIcQCIcRlIcQpIUSzLMdeFkJc0m0v6/sXUBRFUQomP8M504BJUsp/hBD2wHEhRJCU8lyWc3oCNXVbS+B7oKUQwhn4GPAGpO7arVLKKL3+FoqiKEq+5Zn4pZS3gdu6n2OFEMGAO5A18fcH1kjtOo6HhBBlhRAVAR8gSEoZCSCECAJ6AGv1+lvovPTr14RHp2KGFWbSCjNsMZf2aGQZzLFHFM2yBUVRFADqVXLg4771Dd5OgTKhEMILaAocznbIHbiZ5XWobl9u+3N673HAOABPT8+ChJXpTMI6MixScz4oBebYYy6dsJAuWMpyWMpyWGVUwEpWQoNtodpUFEUpbvKd+IUQZYCNwFtSyhh9ByKlXAosBfD29i7UCvBHPJ4joUobEirUJT41ntiUWKKSoohMiiQiMYI7CXe4E3+HW3G3CI09RZpMy7y2vG15ajvXpr5Lfe3mWh9XG1f9/HKKoihFSL4SvxDCAm3S/0lKuSmHU8KAyllee+j2haHt7sm6f29hAs1TYhRWp9Zj9eccnFqMhc7TwKlWrqenZaRxO+42V2OucinqEpcfXOZ85HkOhB0gQ2YAUMWhCk3LNaV5+ea0qtiKCnYVDBK6oiiKMQltt/wTThBCAKuBSCnlW7mc0xt4DeiF9uHuAillC93D3ePAw1E+/wDNH/b558bb21sWaq6e5Fj4fTocXgIO7tBnLtTqVqC3SEhN4HzkeU7eO8k/4f/wb/i/RCdHA+Dl4EWriq1o79GeZyo8g7W5dcFjVBRFMQAhxHEppXe+zs1H4m8L/AmcBjJ0u6cAngBSysW6L4fv0D64TQBGSSmP6a4frTsf4Asp5cq8gip04n/o5hHY+jrcOw8NB0GPGWBXuG6bDJnBpahLHL59mEO3D3Hs7jES0xKx1ljTqmIrulTpgk9lHxytHAsfr6IoylPSa+I3hadO/ABpyXBgLuyfBVb22uTfaDAI8VRvm5yezNE7R9l3cx97Q/dyJ/4O5sKclpVa0qtqLzp7dsbOwu7pYlcURSkglfizCj+