{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to Implementation of a basic CNN\n",
    "\n",
    "<img src=\"https://snag.gy/yjkZ0C.jpg\" >\n",
    "\n",
    "# Minibatch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_lib import get_data, test_near\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "# getting some basic params\n",
    "n, m = x_train.shape\n",
    "n_classes = y_train.max() + 1\n",
    "n_hidden = 50\n",
    "\n",
    "# our simple model from before\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, n_hidden, 10)\n",
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Cross Entropy Loss\n",
    "\n",
    "This is based on softmax\n",
    "\n",
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>output</th>\n",
       "      <th>exp</th>\n",
       "      <th>softmapx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.882275</td>\n",
       "      <td>-0.125251</td>\n",
       "      <td>0.031979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.502540</td>\n",
       "      <td>-0.688080</td>\n",
       "      <td>0.175678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>0.244549</td>\n",
       "      <td>-1.408342</td>\n",
       "      <td>0.359574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fish</td>\n",
       "      <td>0.672065</td>\n",
       "      <td>-0.397400</td>\n",
       "      <td>0.101463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>building</td>\n",
       "      <td>0.273180</td>\n",
       "      <td>-1.297626</td>\n",
       "      <td>0.331306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    output       exp  softmapx\n",
       "0       cat  0.882275 -0.125251  0.031979\n",
       "1       dog  0.502540 -0.688080  0.175678\n",
       "2     plane  0.244549 -1.408342  0.359574\n",
       "3      fish  0.672065 -0.397400  0.101463\n",
       "4  building  0.273180 -1.297626  0.331306"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'label': ['cat', 'dog', 'plane', 'fish', 'building'], 'output': np.random.rand(5)})\n",
    "df['exp']= np.log(df['output'])\n",
    "df['softmapx'] = df['exp'] / df['exp'].sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
    "\n",
    "<img src='https://snag.gy/jQnH5O.jpg' style=\"width:400px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.3949, -2.2743, -2.3425], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the first 3 elements of the truth\n",
    "print(y_train[:3])\n",
    "\n",
    "# compare to our predictions\n",
    "# first 3\n",
    "row_idx = [0, 1, 2]\n",
    "\n",
    "# what was the probabilty for each class?\n",
    "col_idx = [5, 0 ,4]\n",
    "\n",
    "# whats the probability associated with 5, 0 and 4 \n",
    "# an example of advanced indexing\n",
    "sm_pred[row_idx, col_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(inputs, target):\n",
    "    \"\"\" negative log-likelihood \"\"\" \n",
    "    return -inputs[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3053, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
